{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import fasttext\n",
    "import en_core_web_sm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Iterable\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "from torchtext.vocab import FastText\n",
    "from torchtext.data import Example, Field, Dataset, NestedField, BucketIterator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"data/train.jsonl\", lines=True, orient=\"records\")\n",
    "dev = pd.read_json(\"data/dev.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"answer\"] = train[\"answer\"].astype(int)\n",
    "dev[\"answer\"] = dev[\"answer\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEyCAYAAADA/hjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X1w3Hd9J/D3R7tarVa7evTZji0bSUSyLRnLiE2EDtUJtHc4wDh0IJB06JUbqO/I5S5Xjs6EMsMxtAx9oHQGJh0Khfbu2hAeOqUpuM0cPWTXRfFEFpbP8oOsyD5bTmxj62F3pX3Qar/3x64Wx7GtlT8rfXe/fr9mPKOVfv59P37/9PVn9/coxhgQERFR6aiwXQARERG9HpszERFRiWFzJiIiKjFszkRERCWGzZmIiKjEsDkTERGVGDZnIiKiEsPmTEREVGLYnImIiEqM19bA69atMy0tLUVbXyqVgs/nK9r67lXMUY8Z6jFDPWaoV+wMjx49es0Y868KWdZac25pacHQ0FDR1hePx1FdXV209d2rmKMeM9RjhnrMUK/YGYrI/yt0WWd2a09OTtouwQnMUY8Z6jFDPWaoZzNDZ5rzpUuXbJfgBOaoxwz1mKEeM9SzmaEzzZmIiMgV1o4538rCwgImJyeRSCRW/HfXrVuHU6dOrUJVxef3+9Hc3IzKykrbpbzBtm3bbJdQ9pihHjPUY4Z6NjMsqeY8OTmJUCiElpYWiMiK/u7CwkJJNrubGWNw/fp1TE5OorW11XY5b+DxeGyXUPaYoR4z1GOGejYzLKnd2olEAk1NTStuzEt/txyICJqamkq23pMnT9ouoewxQz1mqMcM9WxmWFLNGcBdNeZycy/8G4mI6O6VXHO+W15vSe2hL1sbN260XULZY4Z6zFCPGerZzLCkO9qf/O+xgpc1xiz7ifS3/k3HHX8+MzOD5557Dk8++WTB4wLAe97zHjz33HOor69f0d8rRaV4HLzcMEM9ZqjHDPVsZujMJ+eFhZR6HTMzM/jTP/3TN3w/nU7f8e8dOHDAicYMAIODg7ZLKHvMUI8Z6jFDPZsZlvQn57X2zDPP4JVXXsHu3btRWVkJv9+PhoYGnD59GmNjY3j/+9+PixcvIpFI4Omnn8b+/fsB/OJWpLFYDI888gj6+/vx05/+FJs3b8bf/d3f8RZ6RES0Ig41Z/1JVr//+7+PEydO4NixYxgYGMB73/tenDhxIr9r41vf+hYaGxsRj8fxwAMP4AMf+ACamppet46zZ8/i29/+Nr7xjW/gQx/6EP7mb/4GH/nIR9S1rRW+kdAr9QxXcrjIlg3JxbKos5QxQ71/XWtvLjvTnFfj6SsPPvjg6445fOUrX8Hf/u3fAgAuXryIs2fPvqE5t7a2Yvfu3QCAt73tbTh//nzR61pNvb29tksoe6We4dsvfN12CQXhEVM9ZqjT+7EvWRvbmWPOqZT+mPPNampq8l8PDAzgxz/+MQYHBzEyMoK3vvWtt7xWuaqqKv+1x+NZ9nh1qTly5IjtEsoeM9SbCe2wXULZY4Z6NueyM80ZMOo1hEIhRKPRW/5sdnYWDQ0NCAQCOH36NF566SX1eKUoHo/bLqHsMUO9TEXV8gvRHTFDPZtzuaR3ay936dONotEoQqGQarympia84x3vwM6dO1FdXY0NGzbkf7Z371587Wtfw44dO7Bt2za8/e1vV41FRER0O2KM/hPn3QiHw2ZoaOh13zt16hR27Li7XTGZTAYVFeWzI0Dzb11NyWTydbvmaeVKPcPBb37KdgnLykglKsyC7TLKGjPU6/nIF4o6l0XkqDEmXMiyBXUzEdkrImdEZFxEnrnNMh8SkZMiMioiz62k4GJIJpNrPaSTzp07Z7uEsscM9eL++2yXUPaYoZ7NubxscxYRD4BnATwCoBPAEyLSedMy7QA+DeAdxpguAP91FWq9o3I78apUXb582XYJZY8Z6iV9jbZLKHvMUM/mXC7kk/ODAMaNMRPGmBSA5wE8etMyvwngWWPMNAAYY64Wt0wiIqJ7RyHNeTOAize8nsx970YdADpE5F9E5CUR2VusAgvl9/vXekgndXZ2Lr8Q3REz1KuZP2+7hLLHDPVszuVina3tBdAO4GEAzQAOichbjDEzNy4kIvsB7AeATZs2YWBgAADQ1taGUCiERCKBaDQKj8eD6upqxGKx/N8NhUKYm5tDJpMBAAQCASwsLGBhIXvCw9JTqZauPfZ6vfD7/fl1iAiCweDr1lFTU4NUKpVfR1VVFUTkdeuoqqrC3Nzc69YRi8WwdCJdTU0Nkslkfre63++HMSZ/DLyyshI+ny+/joqKivzfWfr39/f3Y2xsDFevZnc47Ny5E8lkEmfPngUAbNmyBRs2bMDSCXS1tbXo6enB4cOH8+Pu2bMHo6OjuH79OgCgu7sb0WgUExMTALK3GG1sbMTw8DAAoKGhAd3d3Th48GD+oSEPPfQQXnnllfwzTHt6ejA1NZW/kcrSdhoZGQGQPbu9q6sLhw4dyufV39+P4eFhRCIRAEA4HMaVK1dw8WL2/V17ezuqqqpw4sQJAMD69evR0dGBw4cP57dBX19f/naoQPamHpOTk7h06RIAYNu2bfB4PPk6N27ciNbW1vx9cKurq9Hb24sjR47kL4Xo6+vDuXPn8rupOjs7sbi4iDNnzgAANm/ejObm5vx1jcFgEOFwGIODg/ltWeh2GhkZQWVl5apup5GREUxPT9/Vdpqv24WG2eOIBDuQ9gSyv1PR00j5GpGoWg8ACMQnUZFJIVbTBgDwLcwgMH8BM3W7sr/HZgH1kVHMhrZjsSL7xrguehKJqvVI+tYBAGriFwCTwVygJbttU1OoTryGmdqu7DoySdRHT2EmtCN/2U99ZBRx/31I+NdjLpPONhipwFz11tw6rsGfvIrZUPY/TU8mgbroaczUdiEjldl1zB7HfGArUpXZ+90H5yaQqfBhvroZAOBPXoUvNYVIaHv293ZxHrWxMUzX7YLJfV5pmD2GWE0bFry1AIDQ3DjSngDi/k3Z37HEZVSmI4gEs1eUVKZjCM2NY6quG9m7FRo0zo4gWnM/FrzBbMaxMSx4axH3b8yt41V4F+cRrbk/t44IgnMTmK7L3sRIkFFtJ1PhRXwxvqrbaWnXuavbKTYygm3bthXt/72VWPZsbRHpA/A5Y8y7c68/DQDGmC/esMzXABwxxvxF7vU/AXjGGPPy7dZb7LO1i3Ep1Voq1bO1BwYG8PDDD9suo6yVeoblcLb2VN1uNM4es11GWWOGesk3v6+oc3klZ2sX8sn5ZQDtItIK4BKAxwH82k3L/ADAEwD+QkTWIbube6Lwkm/jJ19cfpkcXyoJ+JZ5Z/LOT6+4hM997nMIBoP41KdK/z80IiJyw7LHnI0xaQBPAXgRwCkA3zXGjIrI50VkX26xFwFcF5GTAH4C4LeNMddXq+hb8VR41nI4Z23efPPpBLRSzFCvKnXNdglljxnq2ZzLBV3nbIw5YIzpMMa82Rjzhdz3PmuMeSH3tTHGfNIY02mMeYsx5vnVLPpWPJ7iNecvfOEL6OjoQH9/f/6Y5CuvvIK9e/fibW97G37pl34Jp0+fxuzsLN70pjflj2HPzc1hy5Yt+WPY5ai5udl2CWWPGer5k7zgQ4sZ6tmcy+VzS61lpBaK8+CLo0eP4vnnn8exY8dw4MABvPxy9rD5/v378dWvfhVHjx7Fl770JTz55JOoq6vD7t27cfDgQQDAD3/4Q7z73e9GZWVlUWqxgQ9t0GOGeksnEtHdY4Z6NudySd9b24Z//ud/xq/+6q8iEMieHblv3z4kEgn89Kc/xWOPPZZfbukM3g9/+MP4zne+g3e+8514/vnn8eSTT1qpm4iI3OFMcxaRVVt3JpNBfX09jh1745mP+/btw+/8zu9gamoKR48exbve9a5Vq2MtBINB2yWUPWao58m88XGstDLMUM/mXHZmt7av0leU9ezZswc/+MEPEI/HEY1G8fd///cIBAJobW3F9773PQCAMSZ/zVswGMQDDzyAp59+Gu973/uKeuzbhnC4oLP86Q6YoV5d9LTtEsoeM9SzOZdL+5PzCi59isViRXmX09PTgw9/+MPo7u7G+vXr8cADDwAA/vqv/xqf+MQn8Hu/93tYWFjA448/ju7ubgDZXduPPfZY/qYi5WxwcBB9fX22yyhrzFBvprYL9ZFR22WUNWaoZ3Mul3ZzXoFiPvryM5/5DD7zmc+84fv/+I//eMvlP/jBDxZ1fJv4dC89Zqi3dBcpunvMUM/mXHZmtzYREZErnGnOPAmnOPr7+22XUPaYoV797HHbJZQ9Zqhncy6XXHO+293DSw+rKAelvAt8bGzMdglljxnqzQe22i6h7DFDPZtzuaSas9/vx/Xr1++qeS099afUGWNw/fr1kn3E5dITl+juMUO9pScV0d1jhno253JJnRDW3NyMyclJ/PznP1/x300kEiXb8G7m9/t5i0ciIrqtkmrOlZWVaG1tvau/e+3aNaxbt67IFd17du7cabuEsscM9YJz+ofa3euYoZ7NuVxSu7U1ePlKcTBHPWaol6kozk2F7mXMUI+XUhXB2bNnbZfgBOaoxwz15qt52EeLGerZnMvONGciIiJXONOct2zZYrsEJzBHPWaox2cR6zFDPZtz2ZnmvGHDBtslOIE56jFDPV9qynYJZY8Z6tmcy84056GhIdslOIE56jFDvUhou+0Syh4z1LM5l51pzkRERK5wpjnX1tbaLsEJzFGPGep5F+dtl1D2mKGezbnsTHPu6emxXYITmKMeM9SrjfH+5FrMUM/mXHamOR8+fNh2CU5gjnrMUG+6bpftEsoeM9SzOZedac7l8uCLUscc9ZihnnHnvyZrmKGezbnMrUdERFRinGnOe/bssV2CE5ijHjPUa5g9ZruEsscM9WzOZWea8+joqO0SnMAc9ZihXqymzXYJZY8Z6tmcy8405+vXr9suwQnMUY8Z6i14eTmaFjPUszmXnWnORERErnCmOXd3d9suwQnMUY8Z6oXmxm2XUPaYoZ7NuexMc45Go7ZLcAJz1GOGemlPwHYJZY8Z6tmcy84054mJCdslOIE56jFDvbh/k+0Syh4z1LM5l51pzkRERK5wpjm3tLTYLsEJzFGPGepVJy7bLqHsMUM9m3O5oOYsIntF5IyIjIvIM7f4+UdF5Ociciz35+PFL/XOGhsb13pIJzFHPWaoV5mO2C6h7DFDPZtzednmLCIeAM8CeARAJ4AnRKTzFot+xxizO/fnz4tc57KGh4fXekgnMUc9ZqgXCXbYLqHsMUM9m3O5kE/ODwIYN8ZMGGNSAJ4H8OjqlkVERHTv8hawzGYAF294PQmg9xbLfUBE9gAYA/BbxpiLNy8gIvsB7AeATZs2YWBgAADQ1taGUCiEkZERAEBTUxO6urpw6NChbJFeL/r7+zE8PIxIJLurJhwO48qVK7h4MTuMz+fDtWvXcOLECQDA+vXr0dHRkX/kV1VVFfr6+jA0NIRYLAYA6O3txeTkJC5dugQA2LZtGzweD06ePAkA2LhxI1pbWzE4OAgAqK6uRm9vL44cOYJ4PA4A6Ovrw7lz53D5cvb4TmdnJxYXF3HmzJlseJs3o7m5GUeOHAEABINBhMNhDA4OIplMAgD6+/sxNjaGq1evAgB27tyJZDKJs2fPAgC2bNmCDRs2YGhoCED2AeA9PT04fPhw/qkpe/bswejoaP6ONt3d3YhGo/mzDVtaWtDY2Jh/J9jQ0IDu7m4cPHgQxhiICB566CFkMpn8dunp6cHU1BTOnz9ftO3U3t6Oqqoqp7dTPB7HwMDAqm6nkZERTE9P39V2mq/bhYbZ44gEO/KX29RGTyPla0Siaj0AIBCfREUmlb8FpG9hBoH5C5jJPYawwiygPjKK2dB2LFb4AQB10ZNIVK1H0rcOAFATvwCYDOYCLdltm5pCdeI1zNR2ZdeRSaI+egozoR3IVFQBAOojo4j778OipwpTdbtRM38ekArMVW/NreMa/MmrmA1ld955MgnURU9jprYLGanMrmP2OOYDW5GqrM9uy7kJZCp8mK9uBgD4k1fhS00hEtqe/b1dnEdtbAzTdbvyT3JqmD2GWE1b/i5boblxpD2B/BnQ1YnLqExH8p9OK9MxhObGMVXXDUAAGDTOjiBacz8WvMFsxrExLHhrEfdvzK3jVXgX5xGtuT+3jgiCcxOYrtsNABBkVNtp0VOFmdquVd1OSV92t6+r26kiHseFCxeK9v/eSogx5s4LiHwQwF5jzMdzr38dQK8x5qkblmkCEDPGJEXkPwD4sDHmXXdabzgcNkvNhojWzuA3P2W7BKKy0PexLxV1fSJy1BgTLmTZQnZrXwKw5YbXzbnv5RljrhtjkrmXfw7gbYUMXkwHDx5c6yGdxBz1mKFe9pMNaTBDPZtzuZDm/DKAdhFpFREfgMcBvHDjAiJy3w0v9wE4VbwSC7PcHgAqDHPUY4bFILYLcAAz1LI5l5c95myMSYvIUwBeBOAB8C1jzKiIfB7AkDHmBQD/RUT2AUgDmALw0VWs+ZZE+ItYDMxRjxkWA9/g6DFDLZtzedljzquFx5yJ7OAxZ6LClPox57KwdCYd6TBHPWaot3QGM909Zqhncy4705yXLishHeaoxwz1li5robvHDPVszmVnmjMREZErnGnOPT09tktwAnPUY4Z6tbEx2yWUPWaoZ3MuO9Ocp6ambJfgBOaoxwz1lu74RHePGerZnMvONOelWxeSDnPUY4Z6S7e4pLvHDPVszmVnmjMREZErnGnObW1ttktwAnPUY4Z61YlXbZdQ9pihns257ExzDoVCtktwAnPUY4Z63sV52yWUPWaoZ3MuO9OceeOH4mCOesxQjzfQ0GOGerwJCREREeU505ybmppsl+AE5qjHDPUq0xHbJZQ9Zqhncy4705y7urpsl+AE5qjHDPWCcxO2Syh7zFDP5lx2pjkfOnTIdglOYI56zFBvum637RLKHjPUszmXnWnORERErnCmOXu9XtslOIE56jFDPUHGdglljxnq2ZzLzjTn/v5+2yU4gTnqMUO9htnjtksoe8xQz+ZcdqY5Dw8P2y7BCcxRjxnqRYIdtksoe8xQz+ZcdqY5RyK8bKAYmKMeM9RLewK2Syh7zFDP5lx2pjkTERG5wpnmHA6HbZfgBOaoxwz1aqOnbZdQ9pihns257ExzvnLliu0SnMAc9ZihXsrXaLuEsscM9WzOZWea88WLF22X4ATmqMcM9RJV622XUPaYoZ7NuexMcyYiInKFM825vb3ddglOYI56zFAvEJ+0XULZY4Z6NueyM825qqrKdglOYI56zFCvIpOyXULZY4Z6NueyM835xIkTtktwAnPUY4Z6sZo22yWUPWaoZ3MuO9OciYiIXOFMc16/nmcmFgNz1GOGer6FGdsllD1mqGdzLjvTnDs6eB/ZYmCOesxQLzB/wXYJZY8Z6tmcy84058OHD9suwQnMUY8Z6s3U7bJdQtljhno257IzzZmIiMgVBTVnEdkrImdEZFxEnrnDch8QESMia35DUl6+UhzMUY8Z6lWYBdsllD1mqFfSl1KJiAfAswAeAdAJ4AkR6bzFciEATwM4UuwiC9HX12djWOcwRz1mqFcfGbVdQtljhno253Ihn5wfBDBujJkwxqQAPA/g0Vss97sA/gBAooj1FWxoaMjGsM5hjnrMUG82tN12CWWPGerZnMuFNOfNAG68+/dk7nt5ItIDYIsx5kdFrG1FYrGYraGdwhz1mKHeYoXfdglljxnq2ZzLXu0KRKQCwJcBfLSAZfcD2A8AmzZtwsDAAACgra0NoVAIIyMjAICmpiZ0dXXh0KFD2SK9XvT392N4eBiRSARA9jmbV65cyT81JJVK4dq1a/k7uqxfvx4dHR35s+2qqqrQ19eHoaGhfOC9vb2YnJzEpUuXAADbtm2Dx+PByZMnAQAbN25Ea2srBgcHAQDV1dXo7e3FkSNHEI/HAWR3e5w7dw6XL18GAHR2dmJxcRFnzpwBAGzevBnNzc04ciS7tz8YDCIcDmNwcBDJZBIA0N/fj7GxMVy9ehUAsHPnTiSTSZw9exYAsGXLFmzYsCH/Lq62thY9PT04fPgw0uk0AGDPnj0YHR3F9evXAQDd3d2IRqOYmJgAALS0tKCxsRHDw8MAgIaGBnR3d+PgwYMwxkBE8NBDDyEej+e3S09PD6ampnD+/Pmibaf29nZUVVU5vZ1isRgGBgZWdTuNjIxgenr6rrbTfN0uNMweRyTYgbQnkP2dip5GyteYf5JRID6Jikwqf5cp38IMAvMX8mcAV5gF1EdGMRvanm8CddGTSFStR9K3DgBQE78AmAzmAi3ZbZuaQnXiNczUdmXXkUmiPnoKM6EdyFRkj+3VR0YR99+HtCeAqbrdqJk/D0gF5qq35tZxDf7kVcyGskfWPJkE6qKnMVPbhYxUZtcxexzzga1IVdZnt+XcBDIVPsxXNwMA/Mmr8KWmEMl9svQuzqM2Nobpul0wuc8rDbPHEKtpw4K3FgAQmhtH2hNA3L8p+zuWuIzKdASRYPZSm8p0DKG5cUzVdQMQAAaNsyOI1tyPBW8wm3FsDAveWsT9G3PreBXexXlEa+7PrSOC4NwEput2AwAEGdV2SnsCmKntWtXtlMw9ltLV7WRiMVy4cKFo/++thBhj7ryASB+Azxlj3p17/WkAMMZ8Mfe6DsArAJbeYmwEMAVgnzHmtvsEwuGwKeYug3g8jurq6qKt717FHPVKPcPBb37KdgnLWqzwwcN7Q6swQ73dv/a7RZ3LInLUGFPQCdOF7NZ+GUC7iLSKiA/A4wBeWPqhMWbWGLPOGNNijGkB8BKWacyrYXKST2ApBuaoxwz1+CxiPWaoZ3MuL9ucjTFpAE8BeBHAKQDfNcaMisjnRWTfahdYqKVdnqTDHPWYod7SLle6e8xQz+ZcLuiYszHmAIADN33vs7dZ9mF9WURERPcuZ+4Qtm3bNtslOIE56jFDvZo47wutxQz1bM5lZ5qzx+OxXYITmKMeMywCk7FdQfljhmo257IzzXnpshrSYY56zFBv6bIeunvMUM/mXHamORMREblCfROSUpF+9TgGv/lD22WUvXT1VuaoxAz1qlJTtksoe8xQb+PGjdbGduaTc3XiNdslOIE56jFDPWaoxwz1WltbrY3tTHNeutUc6TBHPWaoxwz1mKHe0i2BbXCmORMREbnCmeZckUnaLsEJzFGPGeoxQz1mqGfzHvnONOf66CnbJTiBOeoxQz1mqMcM9Xp7e62N7UxzngntsF2CE5ijHjPUY4Z6zFBv6RGyNjjTnJeeM0o6zFGPGeoxQz1mqLf0PHgbnGnORERErnCmOddHRm2X4ATmqMcM9ZihHjPU6+vrsza2M8057r/PdglOYI56zFCPGeoxQ71z585ZG9uZ5pz0NdouwQnMUY8Z6jFDPWaod/nyZWtjO9OciYiIXOFMc66ZP2+7BCcwRz1mqMcM9ZihXmdnp7WxnWnOEHf+KVYxRz1mqMcM9Zih2uLiorWxndl6c9VbbZfgBOaoxwz1mKEeM9Q7c+aMtbGdac5ERESucKY5V6Wu2S7BCcxRjxnqMUM9Zqi3efNma2M705z9yau2S3ACc9RjhnrMUI8Z6jU3N1sb25nmPBuyd1adS5ijHjPUY4Z6zFCPD74gIiKiPGeasyeTsF2CE5ijHjPUY4Z6zFAvGAxaG9uZ5lwXPW27BCcwRz1mqMcM9ZihXjgctja2M815prbLdglOYI56zFCPGeoxQ73BwUFrYzvTnDNSabsEJzBHPWaoxwz1mKFeMpm0NrYzzZmIiMgVzjTn+tnjtktwAnPUY4Z6zFCPGer19/dbG9uZ5jwf4H1ki4E56jFDPWaoxwz1xsbGrI3tTHNOVdbbLsEJzFGPGeoxQz1mqHf1qr27rBXUnEVkr4icEZFxEXnmFj//jyLyf0XkmIgcFhHemoaIiOguLducRcQD4FkAjwDoBPDELZrvc8aYtxhjdgP4QwBfLnqlywjOTaz1kE5ijnrMUI8Z6jFDvZ07d1obu5BPzg8CGDfGTBhjUgCeB/DojQsYYyI3vKwBYIpXYmEyFb61HtJJzFGPGeoxQz1mqGfzUipvActsBnDxhteTAHpvXkhE/hOATwLwAXjXrVYkIvsB7AeATZs2YWBgAADQ1taGUCiEkZERAEBTUxO6urpw6NChbJFeL/r7+zE8PIxIJPs+IBwO48qVK7h4MVtaLNCCikwKsZo2AIBvYQaB+QuYqdsFAKgwC6iPjGI2tB2LFX4AQF30JBJV65H0rQMA1MQvACaDuUALAKAqNYXqxGv5i/krMknUR09hJrQDmYoqAEB9ZBRx/31I+hqz65g/D0hF/kHnValr8Cev5m9C78kkUBc9jZnarvx1iPWzxzEf2Jo/RhScm0Cmwof56uwTUfzJq/ClphAJbc/msTiP2tgYput2weTeXzXMHkOspg0L3loAQGhuHGlPAHH/JgBAdeIyKtMRRIIdAIDKdAyhuXFM1XUDEAAGjbMjiNa8OT9ubWwMC95axP0bc+t4Fd7FeURr7s+tI4Lg3ASm63Znty8yaJg9jkiwA2lPILuO6GmkfI1IVK0HAATik+5vp2AH5hebV3k73Y8Fb9DZ7TRX3Yz5xWYH5pO97ZT2BJBIbyj/+WRxO13/2c+wuLio7k/t7e2oqspmVygx5s4fckXkgwD2GmM+nnv96wB6jTFP3Wb5XwPwbmPMb9xpveFw2AwNDa2o2Dv50ff/Co2zx4q2vnvVVN1u5qjEDPWYoR4z1Eu++X14+OGHi7Y+ETlqjCnonqCF7Na+BGDLDa+bc9+7necBvL+QwYuJzy4tDuaoxwz1mKEeM9TbsmXL8gutkkKa88sA2kWkVUR8AB4H8MKNC4hI+w0v3wvgbPFKLIwvNbXWQzqJOeoxQz1mqMcM9TZs2GBt7GWbszEmDeApAC8COAXgu8aYURH5vIjsyy32lIiMisgxZI8733GX9mpYOi5BOsxRjxnqMUM9ZqhXzEOvK1XICWEwxhwAcOCm7332hq+fLnJdRERE9yxn7hDmXZy3XYITmKMeM9RjhnoBGQ8mAAATSUlEQVTMUK+2ttba2M4059qYvXuguoQ56jFDPWaoxwz1enp6rI3tTHOezl1/STrMUY8Z6jFDPWaod/jwYWtjO9OcjTv/FKuYox4z1GOGesxQL51OWxubW4+IiKjEONOcG3gnnKJgjnrMUI8Z6jFDvT179lgb25nmvHRvWdJhjnrMUI8Z6jFDvdHRUWtjO9Ocl258TjrMUY8Z6jFDPWaod/36dWtjO9OciYiIXOFMcw7NjdsuwQnMUY8Z6jFDPWao193dbW1sZ5rz0vNOSYc56jFDPWaoxwz1otGotbGdac5LD9cmHeaoxwz1mKEeM9SbmJiwNrYzzZmIiMgVzjTn6sRl2yU4gTnqMUM9ZqjHDPVaWlqsje1Mc65MR2yX4ATmqMcM9ZihHjPUa2xstDa2M805EuywXYITmKMeM9RjhnrMUG94eNja2M40ZyIiIlc405wr0zHbJTiBOeoxQz1mqMcM9RoaGqyN7Uxz5gX3xcEc9ZihHjPUY4Z6vAlJEUzV2QvRJcxRjxnqMUM9Zqh38OBBa2M705wBsV2AI5ijHjPUY4Z6zFDLGGNtbIeas70Q3cIc9ZihHjPUY4ZaIvbe4DjTnBtnR2yX4ATmqMcM9ZihHjPUe+ihh6yN7Uxzjtbcb7sEJzBHPWaoxwz1mKHeyIi9NzjONOcFb9B2CU5gjnrMUI8Z6jFDvenpaWtjO9OciYiIXOFMc66NjdkuwQnMUY8Z6jFDPWao19PTY21sZ5rzgrfWdglOYI56zFCPGeoxQ72pqSlrYzvTnOP+jbZLcAJz1GOGesxQjxnqnT9/3trYzjRnIiIiVzjTnKsTr9ouwQnMUY8Z6jFDPWao19bWZm1sZ5qzd3HedglOYI56zFCPGeoxQ71QKGRt7IKas4jsFZEzIjIuIs/c4uefFJGTInJcRP5JRN5U/FLvjBfcFwdz1GOGesxQjxnqlfRNSETEA+BZAI8A6ATwhIh03rTYzwCEjTG7AHwfwB8Wu1AiIqJ7RSGfnB8EMG6MmTDGpAA8D+DRGxcwxvzEGLO0D+UlAM3FLXN5lenIWg/pJOaoxwz1mKEeM9RramqyNnYhzXkzgIs3vJ7Mfe92PgbgHzRF3Y3g3MRaD+kk5qjHDPWYoR4z1Ovq6rI2treYKxORjwAIA7jlozxEZD+A/QCwadMmDAwMAMieERcKhfL795uamtDV1YVDhw5li/R60d/fj+HhYUQi2XeD4XAYV65cwcWL2fcN0fowamNjiNVkz67zLcwgMH8BM3W7AAAVZgH1kVHMhrZjscIPAKiLnkSiaj2SvnUAgJr4BcBkMBdoAQBUpaZQnXgNM7XZDVSRSaI+egozoR3IVFQBAOojo4j770PS15hdx/x5QCowV701t45r8CevYjaUPRLgySRQFz2NmdouZKQyu47Z45gPbEWqsh5AdlJlKnyYr87ugPAnr8KXmkIktD2bx+I8amNjmK7bBZN7f9Uwewyxmrb8jQdCc+NIewKI+zcBAKoTl1GZjiAS7AAAVKZjCM2N5x7ILgAMGmdHcK2hF55MEkD2DkML3tr89ZLViVfhXZzPH8uqTEcQnJvAdN3u7PZFBg2zxxEJdiDtCWTXET2NlK8Riar1AIBAfBIVmZTT22mqvgfexflV3U7Rmvvz9052cTvNVTfDuzhf9vPJ5nZKewLwpWfLfj7Z3E4HDhzArl271P2pvb0dVVXZ7Aolyz1MWkT6AHzOGPPu3OtPA4Ax5os3LfcrAL4K4CFjzNXlBg6Hw2ZoaGhFxd7Jj77/V2icPVa09d2rpup2M0clZqjHDPWYoV7yze/Dww8/XLT1ichRY0y4kGUL2a39MoB2EWkVER+AxwG8cNOAbwXwZwD2FdKYV4MgY2NY5zBHPWaoxwz1mKGe11vUncsrsmxzNsakATwF4EUApwB81xgzKiKfF5F9ucX+CEAQwPdE5JiIvHCb1a2ahtnjaz2kk5ijHjPUY4Z6zFCvv7/f2tgFXedsjDlgjOkwxrzZGPOF3Pc+a4x5Iff1rxhjNhhjduf+7LvzGotv6ZgC6TBHPWaoxwz1mKHe8PCwtbGduUPY0gkTpMMc9ZihHjPUY4Z6Syd42eBMcyYiInKFM825NnradglOYI56zFCPGeoxQ71wuKATq1eFM805lbvejnSYox4z1GOGesxQ78qVK9bGdqY5L12UTzrMUY8Z6jFDPWaot3QTERucac5ERESucKY5B+KTtktwAnPUY4Z6zFCPGeq1t7dbG9uZ5lyRSdkuwQnMUY8Z6jFDPWaot9L7YReTM8156cbvpMMc9ZihHjPUY4Z6J06csDa2M82ZiIjIFc40Z9/CjO0SnMAc9ZihHjPUY4Z669fbO+PdmeYcmL9guwQnMEc9ZqjHDPWYoV5Hh737kzvTnJceAk86zFGPGeoxQz1mqHf48GFrYzvTnImIiFzhTHOuMAu2S3ACc9RjhnrMUI8Z6vFSqiKoj4zaLsEJzFGPGeoxQz1mqNfX12dtbGea82xou+0SnMAc9ZihHjPUY4Z6Q0ND1sZ2pjkvVvhtl+AE5qjHDPWYoR4z1IvFYtbGdqY5ExERucKZ5lwXPWm7BCcwRz1mqMcM9ZihXm9vr7WxnWnOfHZpcTBHPWaoxwz1mKHe5KS9J3s505yTvnW2S3ACc9RjhnrMUI8Z6l26dMna2M40ZyIiIlc405xr4ryPbDEwRz1mqMcM9Zih3rZt26yN7UxzhsnYrsANzFGPGeoxQz1mqObxeKyN7Uxzngu02C7BCcxRjxnqMUM9Zqh38qS9M96dac5ERESucKY5V6WmbJfgBOaoxwz1mKEeM9TbuHGjtbGdac7Viddsl+AE5qjHDPWYoR4z1GttbbU2tjPNeaa2y3YJTmCOesxQjxnqMUO9wcFBa2M705yJiIhc4UxzrsgkbZfgBOaoxwz1mKEeM9Srrq62NrYzzbk+esp2CU5gjnrMUI8Z6jFDvZJ/8IWI7BWRMyIyLiLP3OLne0RkWETSIvLB4pe5vJnQDhvDOoc56jFDPWaoxwz1jhw5Ym3sZZuziHgAPAvgEQCdAJ4Qkc6bFrsA4KMAnit2gYXKVFTZGtopzFGPGeoxQz1mqBePx62N7S1gmQcBjBtjJgBARJ4H8CiA/K1TjDHncz/j/eKIiIiUCmnOmwFcvOH1JIC72hEvIvsB7AeATZs2YWBgAADQ1taGUCiEkZERAEBTUxO6urpw6NChbJFeL/r7+zE8PIxIJAIACIfDuHLlCi5ezJZWnbiMlLcWsZo2AIBvYQaB+QuYqdsFAKgwC6iPjGI2tB2LFX4A2YeRJ6rW5x+tVhO/AJhM/rZ3VakpVCdey1+SUJFJoj56CjOhHfl3pfWRUcT99yHpa8yuY/48IBWYq96aW8c1+JNXMRvK7mzwZBKoi57GTG0XMlKZXcfsccwHtiJVWQ8ACM5NIFPhw3x1MwDAn7wKX2oKkdD2bB6L86iNjWG6bhdMbudHw+wxxGrasOCtBQCE5saR9gQQ92/K51OZjiAS7AAAVKZjCM2NY6quG4AAMGicHYF3cR5TdbsBALWxMSx4axH3b8yt41V4F+cRrbk/t44IgnMTmM4tL8igYfY4IsEOpD2B7Dqip5HyNeafLRuIT6Iik3J6O0EEU3W7V3U7RWvux4I36Ox2Wsqw3OeT1e0kgpnarrKfTza3U8AYXLhwQd2f2tvbUVW1sj0ZYoy58wLZY8h7jTEfz73+dQC9xpinbrHsXwL4oTHm+8sNHA6HzdDQ0IqKvZMfP/cVPoWlCOaqtzJHJWaoxwz1mKFewzs+ju3btxdtfSJy1BgTLmTZQk4IuwRgyw2vm3PfKylL7+BIhznqMUM9ZqjHDPUuX75sbexCmvPLANpFpFVEfAAeB/DC6pZFRER071q2ORtj0gCeAvAigFMAvmuMGRWRz4vIPgAQkQdEZBLAYwD+TERGV7PoW6mZP7/WQzqJOeoxQz1mqMcM9To7b74wae0UckIYjDEHABy46XufveHrl5Hd3W2POHM/FbuYox4z1GOGesxQbXFx0drYzmy9pbMESYc56jFDPWaoxwz1zpw5Y21sZ5ozERGRK5xpzlWpa7ZLcAJz1GOGesxQjxnqbd682drYzjRnf/Kq7RKcwBz1mKEeM9RjhnrNzfZOpXKmOS/diYZ0mKMeM9RjhnrMUK+kH3xBREREa8uZ5uzJJGyX4ATmqMcM9ZihHjPUCwaD1sZ2pjnXRU/bLsEJzFGPGeoxQz1mqBcOF3Qb7FXhTHNeeoIK6TBHPWaoxwz1mKHe4OCgtbGdac5LjyEjHeaoxwz1mKEeM9RLJpPWxnamORMREbnCmeZcP3vcdglOYI56zFCPGeoxQ73+/n5rYzvTnOcDvI9sMTBHPWaoxwz1mKHe2NiYtbGdac6pynrbJTiBOeoxQz1mqMcM9a5etXeXNWeaMxERkSucac7BuQnbJTiBOeoxQz1mqMcM9Xbu3GltbGeac6bCZ7sEJzBHPWaoxwz1mKEeL6Uqgvlqe08PcQlz1GOGesxQjxnqnT171trYzjRnIiIiVzjTnPns0uJgjnrMUI8Z6jFDvS1btlgb25nm7EtN2S7BCcxRjxnqMUM9Zqi3YcMGa2M705wjoe22S3ACc9RjhnrMUI8Z6g0NDVkb25nmTERE5ApnmrN3cd52CU5gjnrMUI8Z6jFDvdraWmtjO9Oca2P27oHqEuaoxwz1mKEeM9Tr6emxNrYzzXm6bpftEpzAHPWYoR4z1GOGeocPH7Y2tjPN2bjzT7GKOeoxQz1mqMcM9dLptLWxufWIiIhKjDPNuWH2mO0SnMAc9ZihHjPUY4Z6e/bssTa2M805VtNmuwQnMEc9ZqjHDPWYod7o6Ki1sZ1pzgtee6e8u4Q56jFDPWaoxwz1rl+/bm1sZ5ozERGRKwpqziKyV0TOiMi4iDxzi59Xich3cj8/IiItxS50OaG58bUe0knMUY8Z6jFDPWao193dbW3sZZuziHgAPAvgEQCdAJ4Qkc6bFvsYgGljzP0A/gTAHxS70OWkPYG1HtJJzFGPGeoxQz1mqBeNRq2NXcgn5wcBjBtjJowxKQDPA3j0pmUeBfA/cl9/H8Avi4gUr8zlxf2b1nI4ZzFHPWaoxwz1mKHexMSEtbELac6bAVy84fVk7nu3XMYYkwYwC6CpGAUSERHda7xrOZiI7AewP/cyJiJnirj6dQCuFXF99yrmqMcM9ZihHjNU++NiZ/imQhcspDlfArDlhtfNue/daplJEfECqAPwhnPQjTFfB/D1QotbCREZMsaEV2Pd9xLmqMcM9ZihHjPUs5lhIbu1XwbQLiKtIuID8DiAF25a5gUAv5H7+oMA/o8xxhSvTCIionvHsp+cjTFpEXkKwIsAPAC+ZYwZFZHPAxgyxrwA4JsA/peIjAOYQraBExER0V0o6JizMeYAgAM3fe+zN3ydAPBYcUtbsVXZXX4PYo56zFCPGeoxQz1rGQr3PhMREZUW3r6TiIioxLA5ExERlZiya87lcJ/vUldAhp8UkZMiclxE/klECr42716xXIY3LPcBETEiwktabqGQHEXkQ7nfx1EReW6tayx1BcznrSLyExH5WW5Ov8dGnaVKRL4lIldF5MRtfi4i8pVcvsdFpGdNCjPGlM0fZM8WfwVAGwAfgBEAnTct8ySAr+W+fhzAd2zXXUp/CszwnQACua8/wQxXnmFuuRCAQwBeAhC2XXep/Snwd7EdwM8ANORer7dddyn9KTDDrwP4RO7rTgDnbdddSn8A7AHQA+DEbX7+HgD/AEAAvB3AkbWoq9w+OZfFfb5L3LIZGmN+YoyZz718Cdkbz9AvFPJ7CAC/i+xDYBJrWVwZKSTH3wTwrDFmGgCMMVfXuMZSV0iGBsDSw53rALy6hvWVPGPMIWQvAb6dRwH8T5P1EoB6Eblvtesqt+bM+3zrFZLhjT6G7LtG+oVlM8zt+tpijPnRWhZWZgr5XewA0CEi/yIiL4nI3jWrrjwUkuHnAHxERCaRvST2P69Nac5Y6f+ZRbGm99am8iIiHwEQBvCQ7VrKiYhUAPgygI9aLsUFXmR3bT+M7B6cQyLyFmPMjNWqyssTAP7SGPPHItKH7A2jdhpjMrYLo9srt0/OK7nPN+50n+97WCEZQkR+BcBnAOwzxiTXqLZysVyGIQA7AQyIyHlkj1O9wJPC3qCQ38VJAC8YYxaMMecAjCHbrCmrkAw/BuC7AGCMGQTgR/ahGFSYgv7PLLZya868z7feshmKyFsB/BmyjZnH+N7ojhkaY2aNMeuMMS3GmBZkj9vvM8YM2Sm3ZBUyn3+A7KdmiMg6ZHdz23vIbukpJMMLAH4ZAERkB7LN+edrWmV5ewHAv8udtf12ALPGmNdWe9Cy2q1teJ9vtQIz/CMAQQDfy51Ld8EYs89a0SWmwAxpGQXm+CKAfysiJwEsAvhtYwz3hOUUmOF/A/ANEfktZE8O+yg/sPyCiHwb2TeA63LH5f87gEoAMMZ8Ddnj9O8BMA5gHsC/X5O6uI2IiIhKS7nt1iYiInIemzMREVGJYXMmIiIqMWzOREREJYbNmYiIqMSwORMREZUYNmciIqIS8/8BBVU7wtxA9RgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class distribution   \n",
      "\n",
      "      |   No   |  Yes  \n",
      "-----------------------\n",
      "Train | 0.3769 | 0.6231\n",
      "Dev   | 0.3783 | 0.6217\n"
     ]
    }
   ],
   "source": [
    "# Class distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "weights = np.ones(train.shape[0]) / train.shape[0]\n",
    "train_dist, _, _ = plt.hist(train[\"answer\"], bins=2, weights=weights, label=\"train\", alpha=0.5)\n",
    "\n",
    "weights = np.ones(dev.shape[0]) / dev.shape[0]\n",
    "dev_dist, _, _ = plt.hist(dev[\"answer\"], bins=2, weights=weights, label=\"dev\", alpha=0.5)\n",
    "\n",
    "plt.grid(ls='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"  Class distribution   \")\n",
    "print()\n",
    "print(\"      |   No   |  Yes  \")\n",
    "print(\"-----------------------\")\n",
    "print(\"Train | {:.4f} | {:.4f}\".format(*train_dist))\n",
    "print(\"Dev   | {:.4f} | {:.4f}\".format(*dev_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average question length\n",
      "\n",
      "Train: 43.99\n",
      "Dev  : 43.21\n"
     ]
    }
   ],
   "source": [
    "# Average question length\n",
    "print(\"Average question length\")\n",
    "print()\n",
    "print(\"Train: {:.2f}\".format(train[\"question\"].apply(lambda x: len(x)).agg(np.mean)))\n",
    "print(\"Dev  : {:.2f}\".format(dev[\"question\"].apply(lambda x: len(x)).agg(np.mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average passage length\n",
      "\n",
      "Train: 565.61\n",
      "Dev  : 559.05\n"
     ]
    }
   ],
   "source": [
    "# Average passage length\n",
    "print(\"Average passage length\")\n",
    "print()\n",
    "print(\"Train: {:.2f}\".format(train[\"passage\"].apply(lambda x: len(x)).agg(np.mean)))\n",
    "print(\"Dev  : {:.2f}\".format(dev[\"passage\"].apply(lambda x: len(x)).agg(np.mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Train -----\n",
      "is      4190\n",
      "can     1136\n",
      "does     952\n",
      "are      693\n",
      "do       664\n",
      "did      461\n",
      "was      335\n",
      "has      302\n",
      "will     181\n",
      "Name: question, dtype: int64\n",
      "\n",
      "-----  Dev  -----\n",
      "is      1532\n",
      "can      394\n",
      "does     373\n",
      "are      251\n",
      "do       243\n",
      "did      134\n",
      "has      124\n",
      "was      104\n",
      "will      68\n",
      "Name: question, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Heuristics\n",
    "# In English yes/no answers are usually used with particular question structure.\n",
    "# As we can see, top-9 first question words in train and dev splits are identical.\n",
    "# So heuristic could be the following: select questions, where first word lies in the list.\n",
    "# From the paper we find that they used the following list:\n",
    "# {“did”, “do”, “does”, “is”, “are”, “was”, “were”, “have”, “has”, “can”, “could”, “will”, “would”}\n",
    "print(\"----- Train -----\")\n",
    "print(train[\"question\"].apply(lambda x: x.split()[0]).value_counts()[:9])\n",
    "print()\n",
    "print(\"-----  Dev  -----\")\n",
    "print(dev[\"question\"].apply(lambda x: x.split()[0]).value_counts()[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6217\n"
     ]
    }
   ],
   "source": [
    "# Naive\n",
    "# For every question predict True -- the most represented class in the train split\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = np.ones(dev.shape[0])\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6474\n"
     ]
    }
   ],
   "source": [
    "# Fasttext\n",
    "fasttext_train = pd.DataFrame()\n",
    "fasttext_train[\"answer\"] = \"__label__\" + train[\"answer\"].astype(str)\n",
    "fasttext_train[\"text\"] = train[\"question\"] + \" \" + train[\"passage\"]\n",
    "fasttext_train.to_csv(\"data/fasttext_train.csv\", index=False, sep=\"\\t\")\n",
    "\n",
    "model = fasttext.train_supervised(\"data/fasttext_train.csv\")\n",
    "\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = [int(model.predict(sent)[0][0][-1]) for sent in dev[\"question\"] + \" \"+ dev[\"passage\"]]\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence: str = None) -> np.array:\n",
    "    sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    indexed = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    segments = [1] * len(tokenized)\n",
    "\n",
    "    tokens = torch.tensor([indexed])\n",
    "    segments = torch.tensor([segments])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(tokens, segments)\n",
    "\n",
    "    return torch.mean(out[2][11][0], dim=0).numpy()\n",
    "        \n",
    "\n",
    "def embed_collection(collection: Iterable = None) -> np.array:\n",
    "    return np.array(\n",
    "        [np.mean([embed_sentence(sentence=sent) for sent in sent_tokenize(item)], axis=0) for item in collection]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_passage = np.array(embed_collection(collection=train[\"passage\"]))\n",
    "x_train_question = np.array(embed_collection(collection=train[\"question\"]))\n",
    "x_dev_passage = np.array(embed_collection(collection=dev[\"passage\"]))\n",
    "x_dev_question = np.array(embed_collection(collection=dev[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.hstack((x_train_passage, x_train_question))\n",
    "x_dev = np.hstack((x_dev_passage, x_dev_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6612\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train, train[\"answer\"])\n",
    "\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = model.predict(x_dev)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UKPLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_collection(collection: Iterable = None) -> np.array:\n",
    "    return np.array([np.mean(model.encode(sent_tokenize(item)), axis=0) for item in collection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_passage_ukp = np.array(embed_collection(collection=train[\"passage\"]))\n",
    "x_train_question_ukp = np.array(embed_collection(collection=train[\"question\"]))\n",
    "x_dev_passage_ukp = np.array(embed_collection(collection=dev[\"passage\"]))\n",
    "x_dev_question_ukp = np.array(embed_collection(collection=dev[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ukp = np.hstack((x_train_passage_ukp, x_train_question_ukp))\n",
    "x_dev_ukp = np.hstack((x_dev_passage_ukp, x_dev_question_ukp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train_ukp, train[\"answer\"])\n",
    "\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = model.predict(x_dev_ukp)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence: str = None) -> np.array:\n",
    "    tokens = torch.tensor(tokenizer.encode(sentence, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(tokens)\n",
    "\n",
    "    return torch.mean(out[2][11][0], dim=0).numpy()\n",
    "        \n",
    "\n",
    "def embed_collection(collection: Iterable = None) -> np.array:\n",
    "    return np.array(\n",
    "        [np.mean([embed_sentence(sentence=sent) for sent in sent_tokenize(item)], axis=0) for item in collection]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_passage_gpt = np.array(embed_collection(collection=train[\"passage\"]))\n",
    "x_train_question_gpt = np.array(embed_collection(collection=train[\"question\"]))\n",
    "x_dev_passage_gpt = np.array(embed_collection(collection=dev[\"passage\"]))\n",
    "x_dev_question_gpt = np.array(embed_collection(collection=dev[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_gpt = np.hstack((x_train_passage_gpt, x_train_question_gpt))\n",
    "x_dev_gpt = np.hstack((x_dev_passage_gpt, x_dev_question_gpt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6569\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train_gpt, train[\"answer\"])\n",
    "\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = model.predict(x_dev_gpt)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more \"no\" questions to the training split\n",
    "indices = np.random.choice(np.array(train[train[\"answer\"] == 0].index), size=10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_passage = [aug.augment(passage) for passage in train[\"passage\"][indices]]\n",
    "augmented_question = [aug.augment(question) for question in train[\"question\"][indices]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence: str = None) -> np.array:\n",
    "    sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized = tokenizer.tokenize(sentence)\n",
    "    indexed = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "    segments = [1] * len(tokenized)\n",
    "\n",
    "    tokens = torch.tensor([indexed])\n",
    "    segments = torch.tensor([segments])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(tokens, segments)\n",
    "\n",
    "    return torch.mean(out[2][11][0], dim=0).numpy()\n",
    "        \n",
    "\n",
    "def embed_collection(collection: Iterable = None) -> np.array:\n",
    "    return np.array(\n",
    "        [np.mean([embed_sentence(sentence=sent) for sent in sent_tokenize(item)], axis=0) for item in collection]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug_passage = np.array(embed_collection(collection=augmented_passage))\n",
    "x_train_aug_question = np.array(embed_collection(collection=augmented_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug = np.hstack((x_train_aug_passage, x_train_aug_question))\n",
    "x_train_augmented = np.vstack((x_train, x_train_aug))\n",
    "y_train_augmented = np.append(train[\"answer\"].to_numpy(), train[\"answer\"][indices].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6618\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(x_train_augmented, y_train_augmented)\n",
    "\n",
    "y_true = dev[\"answer\"]\n",
    "y_pred = model.predict(x_dev)\n",
    "print(\"Accuracy: {:.4f}\".format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. DrQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.char_field = NestedField(\n",
    "            Field(batch_first=True, tokenize=list, lower=True),\n",
    "            init_token=\"<SOS>\",\n",
    "            eos_token=\"<EOS>\",\n",
    "            tokenize=\"spacy\",\n",
    "        )\n",
    "        self.word_field = Field(\n",
    "            init_token=\"<SOS>\",\n",
    "            eos_token=\"<EOS>\",\n",
    "            lower=True,\n",
    "            tokenize=\"spacy\",\n",
    "        )\n",
    "        self.target_field = Field(\n",
    "            is_target=True,\n",
    "            sequential=False,\n",
    "            use_vocab=False,\n",
    "        )\n",
    "\n",
    "        self.fields = [\n",
    "                (\"question_char\", self.char_field),\n",
    "                (\"context_char\", self.char_field),\n",
    "                (\"question\", self.word_field),\n",
    "                (\"context\", self.word_field),\n",
    "                (\"answer\", self.target_field),\n",
    "        ]\n",
    "        self.dict_fields = {\n",
    "            \"context\": [(\"context_char\", self.char_field), (\"context\", self.word_field)],\n",
    "            \"question\": [(\"question_char\", self.char_field), (\"question\", self.word_field)],\n",
    "            \"answer\": (\"answer\", self.target_field),\n",
    "        }\n",
    "\n",
    "    def create_dataset(self, path: str = None) -> Dataset:\n",
    "        df = pd.read_json(path, lines=True, orient=\"records\")\n",
    "\n",
    "        data = pd.DataFrame()\n",
    "        data[\"context\"] = df[\"title\"] + \" \" + df[\"passage\"]\n",
    "        data[\"question\"] = df[\"question\"]\n",
    "        data[\"answer\"] = df[\"answer\"]\n",
    "\n",
    "        items = data.to_dict(\"records\")\n",
    "\n",
    "        return Dataset([Example.fromdict(item, fields=self.dict_fields) for item in items], self.fields)\n",
    "\n",
    "    def build(self, train_path: str = None, dev_path: str = None) -> None:\n",
    "        self.train = self.create_dataset(path=train_path)\n",
    "        self.dev = self.create_dataset(path=dev_path)\n",
    "\n",
    "        self.char_field.build_vocab(self.train)\n",
    "        self.word_field.build_vocab(self.train, vectors=FastText(language=\"en\", max_vectors=30000))\n",
    "\n",
    "        pos, ner = [], []\n",
    "        ind2pos, ind2ner = [], []\n",
    "\n",
    "        for data in self.train:\n",
    "            doc = nlp(\" \".join(data.question) + \" \" + \" \".join(data.context))\n",
    "\n",
    "            pos.extend([token.pos_ for token in doc])\n",
    "            ner.extend([token.label_ for token in doc.ents])\n",
    "\n",
    "            ind2pos.extend([(self.word_field.vocab.stoi[str(token)], token.pos_) for token in doc])\n",
    "            ind2ner.extend([(self.word_field.vocab.stoi[str(token)], token.label_) for token in doc.ents])\n",
    "\n",
    "        self.pos_vocab = {tag: i for i, tag in enumerate(set(pos))}\n",
    "        self.ner_vocab = {tag: i + 1 for i, tag in enumerate(set(ner))}\n",
    "        self.ner_vocab[\"<UNK>\"] = 0\n",
    "\n",
    "        self.ind2pos = {tag[0]: self.pos_vocab[tag[1]] for tag in ind2pos}\n",
    "        self.ind2ner = {tag[0]: self.ner_vocab[tag[1]] for tag in ind2ner}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/wiki.en.vec: 6.60GB [48:25, 2.27MB/s]                                \n",
      "  0%|          | 0/30000 [00:00<?, ?it/s]WARNING:torchtext.vocab:Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████| 30000/30000 [00:04<00:00, 7284.50it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "loader.build(train_path=\"data/train.jsonl\", dev_path=\"data/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(loader.train, batch_size=32, shuffle=True, sort_key=lambda x: len(x.context))\n",
    "dev_iter = BucketIterator(loader.dev, batch_size=128, shuffle=True, sort_key=lambda x: len(x.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loader,\n",
    "        weights,\n",
    "        emb_dim: int = None,\n",
    "        lstm_dim: int = None,\n",
    "        hidden_size: int = None,\n",
    "        dropout: float = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loader = loader\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "        \n",
    "        self.lstm_context = nn.LSTM(\n",
    "            input_size=self.lstm_dim,\n",
    "            hidden_size=self.hidden_size // 2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.lstm_question = nn.LSTM(\n",
    "            input_size=self.emb_dim,\n",
    "            hidden_size=self.hidden_size // 2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        self.alpha = nn.Linear(self.emb_dim, 1)\n",
    "        \n",
    "        self.linear = nn.Linear(4 * hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def ner(self, context):\n",
    "        result = torch.zeros((context.shape[0], context.shape[1], len(self.loader.ner_vocab)))\n",
    "\n",
    "        for i in range(context.shape[0]):\n",
    "            for j in range(context.shape[1]):\n",
    "                out = torch.zeros(len(self.loader.ner_vocab))\n",
    "\n",
    "                if context[i][j] not in self.loader.ind2ner:\n",
    "                    out[self.loader.ner_vocab[\"<UNK>\"]] = 1\n",
    "                else:\n",
    "                    out[self.loader.ner_vocab[context[i][j]]] = 1\n",
    "\n",
    "                result[i, j, :] = out\n",
    "\n",
    "        return result\n",
    "\n",
    "    def pos(self, context):\n",
    "        result = torch.zeros((context.shape[0], context.shape[1], len(self.loader.pos_vocab)))\n",
    "\n",
    "        for i in range(context.shape[0]):\n",
    "            for j in range(context.shape[1]):\n",
    "                out = torch.zeros(len(self.loader.pos_vocab))\n",
    "\n",
    "                if context[i][j] not in self.loader.ind2pos:\n",
    "                    out[self.loader.pos_vocab[\"X\"]] = 1\n",
    "                else:\n",
    "                    out[self.loader.pos_vocab[context[i][j]]] = 1\n",
    "\n",
    "                result[i, j, :] = out\n",
    "\n",
    "        return result\n",
    "\n",
    "    def match(self, context, question):\n",
    "        result = torch.zeros((context.shape[0], context.shape[1], 1))\n",
    "\n",
    "        for i in range(context.shape[0]):\n",
    "            for j in range(context.shape[1]):\n",
    "                out = 1 if context[i][j] in question[i] else 0\n",
    "                result[i, j] = torch.tensor(out)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def align(self, context_emb, question_emb):\n",
    "        value = torch.exp(F.leaky_relu(self.alpha(context_emb.float())))\n",
    "\n",
    "        context = torch.tensor([value for _ in range(len(question_emb))])\n",
    "        question = torch.tensor([F.leaky_relu(self.alpha(q.float())) for q in question_emb])\n",
    "\n",
    "        total = torch.sum(context * question, dim=0)\n",
    "        out = torch.sum((context * question)[:, None] * question_emb / total, dim=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def attention(self, context_emb, question_emb):\n",
    "        result = torch.zeros((context_emb.shape[0], context_emb.shape[1], self.emb_dim))\n",
    "\n",
    "        for i in range(context_emb.shape[0]):\n",
    "            for j in range(context_emb.shape[1]):\n",
    "                result[i, j] = self.align(context_emb[i][j], question_emb[i])\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, batch):\n",
    "        context = batch.context.transpose(0, 1)\n",
    "        question = batch.question.transpose(0, 1)\n",
    "\n",
    "        context_emb = self.emb(context)\n",
    "        question_emb = self.emb(question)\n",
    "\n",
    "        context_ner = self.ner(context)\n",
    "        context_pos = self.pos(context)\n",
    "        context_match = self.match(context, question)\n",
    "        context_attention = self.attention(context_emb, question_emb)\n",
    "        context_features = torch.cat((\n",
    "            context_emb, context_ner, context_pos, context_attention, context_match\n",
    "        ), dim=2)\n",
    "        context_features = self.dropout(context_features)\n",
    "\n",
    "        _, context_out = self.lstm_context(context_features)\n",
    "        _, question_out = self.lstm_question(question_emb)\n",
    "\n",
    "        batch_size = batch.context.shape[1]\n",
    "\n",
    "        context_out = torch.cat((\n",
    "            context_out[0].permute(1, 0, 2).reshape(batch_size, self.hidden_size),\n",
    "            context_out[1].permute(1, 0, 2).reshape(batch_size, self.hidden_size)\n",
    "        ), dim=1)\n",
    "        question_out = torch.cat((\n",
    "            question_out[0].permute(1, 0, 2).reshape(batch_size, self.hidden_size),\n",
    "            question_out[1].permute(1, 0, 2).reshape(batch_size, self.hidden_size)\n",
    "        ), dim=1)\n",
    "\n",
    "        features = torch.cat((context_out, question_out), dim=1)\n",
    "\n",
    "        out = self.linear(self.flatten(features))\n",
    "        out = self.dropout(out)\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def on_epoch_begin(self, is_train, name, batches_count) -> None:\n",
    "        self.epoch_loss = 0\n",
    "        self.correct_count, self.total_count = 0, 0\n",
    "        self.is_train = is_train\n",
    "        self.name = name\n",
    "        self.batches_count = batches_count\n",
    "        self.model.train(is_train)\n",
    "        \n",
    "    def on_epoch_end(self) -> str:\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self.name, self.epoch_loss / self.batches_count, self.correct_count / self.total_count\n",
    "        )\n",
    "        \n",
    "    def on_batch(self, batch) -> str:\n",
    "        logits = self.model(batch)\n",
    "        target = batch.answer\n",
    "        prediction = torch.max(logits, axis=1)[1]\n",
    "\n",
    "        loss = self.criterion(logits, target)\n",
    "\n",
    "        self.total_count += prediction.size(0)\n",
    "        self.correct_count += torch.sum(prediction == target).item()\n",
    "\n",
    "        if self.is_train:\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.epoch_loss += loss.item()\n",
    "\n",
    "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "            self.name, loss.item(), torch.sum(prediction == target).item() / prediction.size(0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.get_lock().locks = []\n",
    "\n",
    "def do_epoch(\n",
    "    trainer: ModelTrainer = None,\n",
    "    data_iter: BucketIterator = None,\n",
    "    is_train: bool = None,\n",
    "    name: str = None\n",
    ") -> None:\n",
    "    trainer.on_epoch_begin(is_train=is_train, name=name, batches_count=len(data_iter))\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=trainer.batches_count) as progress_bar:\n",
    "            for i, batch in enumerate(data_iter):\n",
    "                batch_progress = trainer.on_batch(batch=batch)\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description(batch_progress)\n",
    "\n",
    "            epoch_progress = trainer.on_epoch_end()\n",
    "\n",
    "            progress_bar.set_description(epoch_progress)\n",
    "            progress_bar.refresh()\n",
    "\n",
    "def fit(\n",
    "    trainer: ModelTrainer = None,\n",
    "    train_iter: BucketIterator = None,\n",
    "    epochs_count: int = None,\n",
    "    dev_iter: BucketIterator = None\n",
    ") -> None:\n",
    "    best_val_loss = None\n",
    "\n",
    "    for epoch in range(epochs_count):\n",
    "        try:\n",
    "            name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "            do_epoch(trainer=trainer, data_iter=train_iter, is_train=True, name=name_prefix + 'Train:')\n",
    "\n",
    "            if not dev_iter is None:\n",
    "                do_epoch(trainer=trainer, data_iter=dev_iter, is_train=False, name=name_prefix + '  Val:')\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Early stopping\")\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56e197b1fef401aaf06cda2922edb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44f896d2e5b48f2a231877617525cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658ff7da70164e798e219a1cbd60ef30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ff53b1f3d749dea30f1b88032e3aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c876a4ead94fa490a42c37c2b6440a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9b6d455b3a4bceba60037f80cc4839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2bd221b3ea461b94a07d207a54a079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb99a2db2c54b1eac5b57836381f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4256e2d3f24280930713107a04898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606633a8af70415dbd3d8cd861a0385a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48efbed27e1433e9a179db2e2badb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea415e5858994e16bbf10968744d4932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc20beccbaa4081b66c1009d7a497e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8993d3e28e04404cb2a2fd3d22b832af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aece5ddd9fad43c6aecd85335967c60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac84898f2d804d3e8060647605d0989b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ad43aa17e74af49dd948abdbef23de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe58a5151054604849384ca76df4d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70c341a4540c45fbb89ffdd10e1504c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df2ed56bfa549339de40e400e2e7566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights = loader.word_field.vocab.vectors\n",
    "model = Model(loader=loader, weights=weights, emb_dim=300, lstm_dim=637, hidden_size=128, dropout=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(model.parameters(), lr=1e-3)\n",
    "trainer = ModelTrainer(model=model, criterion=criterion, optimizer=optimizer)\n",
    "fit(trainer=trainer, train_iter=train_iter, epochs_count=10, dev_iter=dev_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. BiDAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights,\n",
    "        char_vocab_size: int = None,\n",
    "        char_emb_dim: int = None,\n",
    "        char_hidden_size: int = None,\n",
    "        char_kernel_size: int = None,\n",
    "        emb_dim: int = None,\n",
    "        hidden_size: int = None,\n",
    "        dropout: float = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        self.char_hidden_size = char_hidden_size\n",
    "        self.char_kernel_size = char_kernel_size\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.char_emb = nn.Embedding(self.char_vocab_size, self.char_emb_dim)\n",
    "        self.word_emb = nn.Embedding.from_pretrained(weights, freeze=True)\n",
    "\n",
    "        self.char_conv = nn.Conv2d(1, self.char_hidden_size, (self.char_emb_dim, self.char_kernel_size))\n",
    "\n",
    "        self.alpha = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(6 * self.hidden_size, 1)\n",
    "        )\n",
    "\n",
    "        self.contextual_lstm = nn.LSTM(\n",
    "            input_size=self.emb_dim + self.char_hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.modeling_lstm_first = nn.LSTM(\n",
    "            input_size=8 * self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.modeling_lstm_second = nn.LSTM(\n",
    "            input_size=2 * self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=10 * hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(4 * hidden_size, 2)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def embed(self, batch):\n",
    "        batch_size = batch.size(0)\n",
    "\n",
    "        emb = self.char_emb(batch)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        emb = emb.transpose(2, 3)\n",
    "        emb = emb.view(-1, self.char_emb_dim, emb.size(3)).unsqueeze(1)\n",
    "\n",
    "        emb = self.char_conv(emb).squeeze()\n",
    "        emb = F.max_pool1d(emb, emb.size(2)).squeeze()\n",
    "\n",
    "        emb = emb.view(batch_size, -1, self.char_hidden_size)\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def attention(self, context, question):\n",
    "        tensor = torch.cat([\n",
    "            context.unsqueeze(2).expand(context.size(0), context.size(1), question.size(1), -1),\n",
    "            question.unsqueeze(1).expand(context.size(0), context.size(1), question.size(1), -1),\n",
    "            context.unsqueeze(2) * question.unsqueeze(1)\n",
    "        ], dim=-1)\n",
    "        s = self.alpha(tensor).squeeze()\n",
    "\n",
    "        a = F.softmax(s, dim=2)\n",
    "        context_question_attention = torch.bmm(a, question)\n",
    "\n",
    "        b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
    "        question_context_attention = torch.bmm(b, context).squeeze()\n",
    "        question_context_attention = question_context_attention.unsqueeze(1).expand(-1, context.size(1), -1)\n",
    "\n",
    "        result = torch.cat([\n",
    "                      context,\n",
    "                      context_question_attention,\n",
    "                      context * context_question_attention,\n",
    "                      context * question_context_attention\n",
    "        ], dim=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def forward(self, batch):\n",
    "        context_char_emb = self.embed(batch.context_char)\n",
    "        question_char_emb = self.embed(batch.question_char)\n",
    "        \n",
    "        context_word_emb = self.word_emb(batch.context.transpose(0, 1))\n",
    "        question_word_emb = self.word_emb(batch.question.transpose(0, 1))\n",
    "\n",
    "        context = torch.cat([context_char_emb, context_word_emb], dim=-1)\n",
    "        question = torch.cat([question_char_emb, question_word_emb], dim=-1)\n",
    "\n",
    "        context, _ = self.contextual_lstm(context)\n",
    "        question, _ = self.contextual_lstm(question)\n",
    "\n",
    "        g = self.attention(context, question)\n",
    "\n",
    "        features, _ = self.modeling_lstm_first(g)\n",
    "        features, _ = self.modeling_lstm_second(features)\n",
    "\n",
    "        _, features = self.lstm(torch.cat([g, features], dim=-1))\n",
    "        features = torch.cat((\n",
    "            features[0].permute(1, 0, 2).reshape(batch.context.size(1), 2 * self.hidden_size),\n",
    "            features[1].permute(1, 0, 2).reshape(batch.context.size(1), 2 * self.hidden_size)\n",
    "        ), dim=1)\n",
    "\n",
    "        out = self.out(features)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130a5d56bd024853a72567562cb1edab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29db0c69350040d586efd792dc7e30c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baede130e8a5436e9db26bd1e1123a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e7034fd9b54f13b2bc579cf42c3143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5bf0a3906b4883873d31d8d18a6314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf37464efb9414a8b6ba63422080c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae948c93a9640608ab47e376fbe4737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b57be0c05a54f128196f2e2c04995fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab05e187c7d42f79d292f7dd419d9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b6d1e4c4b94d34adb7de198c1f6f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79caa03c9e9544d194c630f8823bb08c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ca05a5efb448479943b26ca91659ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9d70f0b5dd4bc29c91f021acc47944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91940f25a6314b46b21f4ae8d0ac61c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4933530e05474aed9f08ea571f790eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14d8a9c8e24422890bd86b64373fa79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114b0e27e76240afb8070768404900ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b38dba52254cf1af8837e0a1695f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237e40841e1446d0be12fe76a7a1584f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=295.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2283ec6d6b04f8ba6d55a6e7e2ff8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights = loader.word_field.vocab.vectors\n",
    "model = Model(\n",
    "    weights=weights,\n",
    "    char_vocab_size=len(loader.char_field.vocab),\n",
    "    char_emb_dim=15,\n",
    "    char_hidden_size=15,\n",
    "    char_kernel_size=5,\n",
    "    emb_dim=300,\n",
    "    hidden_size=64,\n",
    "    dropout=0.3\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "trainer = ModelTrainer(model=model, criterion=criterion, optimizer=optimizer)\n",
    "fit(trainer=trainer, train_iter=train_iter, epochs_count=10, dev_iter=dev_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Baseline\n",
    "\n",
    "Если предсказывать метку самого популярного класса в обучающей выборке, то можно добиться `accuracy=0.6217`.\n",
    "\n",
    "### FastText Baseline\n",
    "\n",
    "Если использовать FastText-эмбеддинги и логистическую регрессию, то можно добиться `accuracy=0.6486`. Возможно, улучшить результаты бейзлайнов можно было бы с помощью подбора параметроа логистической регрессии и нормализации данных: иногда встречаются символы не из английского алфавита -- FastText к такому вряд ли готов.\n",
    "\n",
    "### BERT Embeddings\n",
    "\n",
    "Один из самых успешных способов -- использовать BERT Embeddings и логистическую регрессию. С помощью этого подхода получаем `accuracy=0.6612`.\n",
    "\n",
    "### GPT2 Embeddings\n",
    "\n",
    "Все то же самое, что и в **BERT Embeddings**, только другая модель. Результаты получаются немного хуже: `accuracy=0.6569`.\n",
    "\n",
    "### UKPLab Embeddings\n",
    "\n",
    "Попробовал взять эмбеддинги из другого популярного фреймворка (используемая модель -- Cased BERT, обученный на NLI датасете). Прироста качества не наблюдается: `accuracy=0.6569`.\n",
    "\n",
    "### Augmentation\n",
    "\n",
    "Попробовал сбалансировать обучающую выборку: добавил больше отрицательных примеров (аугментация состоит в замене некоторых слов на ближайшие на основе их векторного представления, полученного с помощью эмбеддингов из Uncased BERT; использовал готовый фреймворк). Немного перебрал размер аугментации: добавив 10 примеров качество увеличилось: `accuracy=0.6615`.\n",
    "\n",
    "### DrQA\n",
    "\n",
    "Архитектура почти такая же, как и в оригинальной статье:\n",
    "1. Эмбеддинг контекста:\n",
    "    - FastText-эмбеддинги\n",
    "    - OneHotEncoded-информация о NER-тегах\n",
    "    - OneHotEncode-информация о POS-тегах\n",
    "    - Выровненные с помощью SoftAttention FastText-эмбеддинги контекста и вопроса\n",
    "2. Эмбеддинг вопроса:\n",
    "    - FastText-эмбеддинги\n",
    "3. Полученные в п.п.1-2 эмбеддинги подаются в BiLSTM (отдельные LSTM для контекста и вопроса). Output этих LSTM конкатенируются -- получили финальные пирзнаки.\n",
    "4. Слой предсказания -- два линейных слоя\n",
    "\n",
    "В такой реализации получается следующее качество: `accuracy=0.6508`\n",
    "\n",
    "Думаю из-за того, что архитектура была придумана для SQuAD, в котором примерно в 8 раз больше данных, обучаться надо с помощью SGD с маленьким learning rate. Также поэкспериментировать с нормализационными слоями и дропаутом. Иначе будем быстро переобучаться \n",
    "\n",
    "### BiDAF\n",
    "\n",
    "Архитектура почти такая же, как и в оригинальной статье:\n",
    "1. Эмбеддинг контекста и вопроса:\n",
    "    - Символьные эмбеддинги (использую CNN)\n",
    "    - FastText-эмбеддинги\n",
    "2. BiLSTM для получения contextual representation\n",
    "3. Отдельный слой внимания\n",
    "4. Слой модельной области: использует п.п.2-3\n",
    "5. Выходной слой -- LSTM (вход -- конкатенация пп.3-4) и Linear\n",
    "\n",
    "В такой реализации получается следующее качество: `accuracy=0.6691`\n",
    "\n",
    "Проблема такая же, как и в DrQA -- необходимо аккуратно обучать модель, чтобы избежать переобучения.\n",
    "\n",
    "### Overall\n",
    "\n",
    "В целом все модели показывают примерно одинаковые результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
